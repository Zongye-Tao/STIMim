# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
from arguments import *
from dataloader import UnifiedDataLoader
args = AttnWGAIN_arguments()
# í ½í´¹ **ç”ŸæˆéšæœºäºŒå€¼çŸ©é˜µå¹¶ç”¨ softmax å¤„ç†**
def M_prob_gen(M_matrix):
    # å°† M_matrix å±•å¹³ä¸ºä¸€ç»´ (batch_size * height * width)
    M_matrix_flatten = torch.flatten(M_matrix)  # å±•å¹³ä¸ºä¸€ç»´

    # å°† M_matrix_flatten è½¬æ¢ä¸º one-hot ç¼–ç 
    M_matrix_one_hot = F.one_hot(M_matrix_flatten.to(torch.int64), num_classes=2)  # (batch_size * height * width, 2)

    # ä½¿ç”¨æ¸©åº¦å‚æ•°çš„ softmax ç”Ÿæˆå¹³æ»‘çš„æ¦‚ç‡åˆ†å¸ƒ
    temperature = torch.rand(M_matrix_one_hot.shape[0], 1, device=M_matrix.device)  # (batch_size * height * width, 1)
    temperature = temperature.expand(M_matrix_one_hot.shape)  # æ‰©å±•åˆ°ä¸ one-hot ç›¸åŒçš„å½¢çŠ¶

    # å°† M_matrix_one_hot ç§»åŠ¨åˆ°ä¸ temperature ç›¸åŒçš„è®¾å¤‡
    M_matrix_one_hot = M_matrix_one_hot.to(temperature.device)
    M_matrix_one_hot_prob = F.softmax(M_matrix_one_hot / temperature, dim=1)  # (batch_size * height * width, 2)

    # æå–æœ€å¤§å€¼å’Œæœ€å°å€¼æ¦‚ç‡
    M_matrix_one_hot_prob_max, _ = torch.max(M_matrix_one_hot_prob, dim=1)  # (batch_size * height * width)
    M_matrix_one_hot_prob_min, _ = torch.min(M_matrix_one_hot_prob, dim=1)  # (batch_size * height * width)

    # æ ¹æ®åŸå§‹å€¼é€‰æ‹©æ¦‚ç‡
    M_matrix_prob = torch.where(M_matrix_flatten == 1, M_matrix_one_hot_prob_max, M_matrix_one_hot_prob_min)
    M_matrix_prob = M_matrix_prob.reshape(M_matrix.shape)  # æ¢å¤ä¸ºåŸå§‹çš„ä¸‰ç»´å½¢çŠ¶
    return M_matrix_prob

def new_train(dataloader):
    evalMask_collector = []  # æ”¶é›†æŒ‡ç¤ºæ©ç 
    for idx, data in enumerate(dataloader):
        indices, X, missing_mask, H, deltaPre, X_holdout, indicating_mask = map(lambda x: x.to(args.device), data)
        evalMask_collector.append(indicating_mask)
        print("indicating_mask: ", indicating_mask.shape, torch.min(indicating_mask), torch.max(indicating_mask))
        #print("indicating_mask number: ", indicating_mask)
    evalMask_collector = torch.cat(evalMask_collector)
    print("evalMask_collector: ", evalMask_collector.shape, torch.min(evalMask_collector), torch.max(evalMask_collector))
    #print("evalMask_collector number: ", evalMask_collector)
    # è¿”å›æ”¶é›†çš„æ•°æ®
    return evalMask_collector
    
# **å®šä¹‰ Variational Autoencoder (VAE)**
class VAE(nn.Module):
    def __init__(self, data_sample_shape=(300, 128), latent_dim=128):
        super(VAE, self).__init__()
        self.data_sample_shape = data_sample_shape
        self.input_dim = data_sample_shape[0] * data_sample_shape[1]
        self.latent_dim = latent_dim
        # print("input_dim: ", self.input_dim, "latent_dim: ", self.latent_dim)

        # ç¼–ç å™¨
        self.fc1 = nn.Linear(self.input_dim, 64)
        self.fc21 = nn.Linear(64, self.latent_dim)  # å‡å€¼
        self.fc22 = nn.Linear(64, self.latent_dim)  # æ–¹å·®

        # è§£ç å™¨
        self.fc3 = nn.Linear(self.latent_dim, 1024)
        self.fc4 = nn.Linear(1024, self.input_dim)
    def encode(self, x):
        h1 = F.leaky_relu(self.fc1(x), negative_slope=0.01)
        mu = self.fc21(h1)
        logvar = self.fc22(h1)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h3 = F.leaky_relu(self.fc3(z), negative_slope=0.01)
        recon_x = torch.sigmoid(self.fc4(h3))
        recon_x = recon_x.view(-1, self.data_sample_shape[0], self.data_sample_shape[1])
        return recon_x

    def forward(self, x):
        x = x.view(-1, self.input_dim)
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar


# **VAE æŸå¤±å‡½æ•°**
def loss_function(recon_x, x, mu, logvar):
    #BCE = nn.BCELoss(reduction='mean')(recon_x, x)  # è®¡ç®—é‡æ„è¯¯å·®
    mse_loss = nn.MSELoss()(recon_x, x)  # ä½¿ç”¨ MSELoss æ›¿ä»£ BCELoss
    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')  # é‡æ„æŸå¤±ï¼Œäº¤å‰ç†µ
    D_KL = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())  # è®¡ç®— KL æ•£åº¦
    return mse_loss, BCE, D_KL, BCE + D_KL


# **è®­ç»ƒ VAE**
def train_vae(vae, M_prob, num_epochs=10000, batch_size=32):
    optimizer = optim.Adam(vae.parameters(), lr=1e-4, weight_decay=1e-6)
    # åˆ›å»ºæ•°æ®é›†
    dataset = TensorDataset(M_prob, M_prob)  # é©î†½çˆ£æ¶”ç†¸æ§¸ M_prob
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    # æ—©åœæŠ€æœ¯ç›¸å…³å˜é‡
    best_rmse_loss = float('inf')
    patience_counter = 0
    early_stop = False
    patience = 1000
    for epoch in range(num_epochs):
        rmse_total = 0
        train_loss = 0
        for batch in train_loader:
            batch_rmse = 0
            data, target = batch
            optimizer.zero_grad()
            recon_x, mu, logvar = vae(data)
            mse_loss, BCE, D_KL, loss = loss_function(recon_x, target, mu, logvar)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            batch_rmse += torch.sqrt(torch.mean((recon_x - target) ** 2)).item()
        # æ¯ 100 ä¸ª epoch è¾“å‡ºä¸€æ¬¡
        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch + 1}, Loss: {train_loss / len(train_loader):.4f}, RMSE: {batch_rmse / len(train_loader):.4f}, mse_loss: {mse_loss:.4f}")
        
        if batch_rmse < best_rmse_loss:
            best_rmse_loss = batch_rmse
            patience_counter = 0
            torch.save(vae.state_dict(), "vae_model.pth")
            #print(f"Epoch {epoch + 1}: æ¨¡å‹å·²ä¿å­˜ï¼ˆæœ€ä½³æŸå¤±: {best_rmse_loss:.4f}ï¼‰")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                early_stop = True
                print(f"Epoch {epoch + 1}: æ—©åœè§¦å‘ï¼Œè®­ç»ƒç»“æŸã€‚")
                break
    #torch.save(vae.state_dict(), "vae_model.pth")
    if not early_stop:
        print("è®­ç»ƒå®Œæˆï¼")
    print("VAE model saved successfully.")
    return vae


def generate_multiple_samples(model, num_samples=10):
    model.eval()
    z_samples = torch.randn(num_samples, model.latent_dim)
    generated_samples = model.decode(z_samples)
    # print("generated_samples: ", generated_samples.shape)
    return generated_samples



if __name__ == "__main__":
    args = AttnWGAIN_arguments()
    #
    unified_dataloader = UnifiedDataLoader(args.data_path, args.seq_len, args.feature_num, args.model_type,
                                            args.hint_rate,
                                            args.miss_rate, args.batch_size, args.num_workers, args.MIT)
    #
    test_dataloader = unified_dataloader.get_test_dataloader()
    #
    device = torch.device("cuda")
    print(f"Current device: {device}")  # è¾“å‡ºå½“å‰è®¾å¤‡î˜¬
    #
    # # è°ƒç”¨ new_train è·å– evalMask_collector
    evalMask_collector = new_train(test_dataloader)
    #
    # # å°† evalMask_collector ä¼ é€’ç»™ M_prob_gen3
    # M_matrix, M_matrix_prob = M_prob_gen(evalMask_collector)

    size = (100, 300, 128)  # åŸå§‹è¾“å…¥ï¼ˆB, L, Kï¼‰
    # data_sample_shape = (300, 128)
    data_sample_shape = (evalMask_collector.shape[1], evalMask_collector.shape[2])
    # M_matrix = torch.randint(0, 2, size, dtype=torch.float32)  # ç”Ÿæˆä¸€ä¸ªéšæœºçš„äºŒå€¼çŸ©é˜µ (0 æˆ– 1)  # ç”Ÿæˆéšæœº 0/1 äºŒå€¼çŸ©é˜µ
    M_matrix = evalMask_collector
    M_matrix_prob = M_prob_gen(M_matrix)
    print("M_matrix: ", M_matrix.shape, "M_matrix_prob: ", M_matrix_prob.shape)

    # åˆå§‹åŒ–VAEæ¨¡å‹
    latent_dim = 256
    vae = VAE(data_sample_shape, latent_dim).to(device)  # é–«å‚å¤
    # train VEA model
    print("Starting VAE training...")
    vae = train_vae(vae, M_matrix_prob, num_epochs=10000, batch_size=256)  # ç’î… ç²Œ VAE
    print("VAE training finished.")

    # # åŠ è½½ VAE æ¨¡å‹æƒé‡ï¼Œå…è®¸éƒ¨åˆ†å±‚çš„æƒé‡ä¸åŒ¹é…
    try:
        vae.load_state_dict(torch.load("vae_model.pth", weights_only=True), strict=False)
        vae.to("cpu")
        generated_samples = generate_multiple_samples(vae, num_samples=evalMask_collector.shape[0])
        print("VAE model loaded successfully.")
        print("generated_samples: ", generated_samples.shape, generated_samples[0, 0, :10])
        train_M_matrix = torch.where(generated_samples > 0.5, torch.tensor(1.0), torch.tensor(0.0))
        print("train missing ratio: ", torch.sum(train_M_matrix) / train_M_matrix.numel())
    except Exception as e:
        print(f"Error loading model: {e}")





